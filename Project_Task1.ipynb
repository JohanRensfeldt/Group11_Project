{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c753a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task: \n",
    "## (1) Which keywords can let your comments get a higher score in Reddit? \n",
    "## 2. Compared the run time of (1) in common way and pyspark\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import os\n",
    "from pyspark.sql.functions import length\n",
    "from Pre_processing import data  # import data from Pre_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b71de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/02 12:04:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# preparation for Task(1) and (2)\n",
    "start_time = time.time()\n",
    "stop_words = ['a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it',\n",
    "              'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they',\n",
    "              'this', 'to', 'was', 'will', 'with', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', \n",
    "              '_', '+', '=', '[', ']', '{', '}', ';', ':', '\"', \"'\", '<', '>', ',', '.', '/', '?', '\\\\', \n",
    "              '|', '`', '~', '']\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    ".master(\"spark://192.168.2.70:7077\") \\\n",
    ".appName(\"yufengchen_app\")\\\n",
    ".config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    ".config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    ".config(\"spark.shuffle.service.enabled\", True)\\\n",
    ".config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    ".config(\"spark.cores.max\", 4)\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40cfc2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, split, explode, sum, count, avg, col\n",
    "\n",
    "\n",
    "def use_pyspark_function(data, spark_session=spark_session):\n",
    "    word_score = {}\n",
    "    word_count = {}\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"ERROR\")  \n",
    "    data_pyspark = spark_session.createDataFrame(data)\n",
    "    print('Using Pyspark')\n",
    "    start_time = time.time()                                                                                           \n",
    "    data_pyspark = data_pyspark.select('score', 'body').filter((length('body') >= 10))\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    word_score = data_pyspark.rdd.flatMap(lambda row: [(word, row['score']) for word in row['body'].split() if word not in stop_words]) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .collectAsMap()\n",
    "    word_count = data_pyspark.rdd.flatMap(lambda row: [(word, 1) for word in row['body'].split() if word not in stop_words]) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .collectAsMap()\n",
    "\n",
    "\n",
    "    # filter the word appear less than 10 times\n",
    "    filtered_word_count = {k: v for k, v in word_count.items() if v >= 10}\n",
    "    filtered_word_score = {k: v for k, v in word_score.items() if k in filtered_word_count.keys()}\n",
    "\n",
    "\n",
    "    for word,times in filtered_word_count.items():\n",
    "        if filtered_word_score[word] != 0:\n",
    "            filtered_word_score[word] = filtered_word_score[word]/times\n",
    "            \n",
    "   \n",
    "    sorted_filtered_word_score = dict(sorted(filtered_word_score.items(), key=lambda x: x[1], reverse=True))\n",
    "    # top 10 word  that may let your comments get a higher score in Reddit\n",
    "    for key, value in list(sorted_filtered_word_score.items())[:10]:\n",
    "        print(f\"{key}: {value}\")\n",
    "    \"\"\"  \n",
    "    \n",
    "    broadcast_stop_words = spark_context.broadcast(stop_words)\n",
    "\n",
    "    # Compute the word score and count RDDs\n",
    "    word_score_rdd = data_pyspark.rdd.flatMap(\n",
    "        lambda row: [(word, row['score']) for word in row['body'].split() if word not in broadcast_stop_words.value]\n",
    "    ).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    word_count_rdd = data_pyspark.rdd.flatMap(\n",
    "        lambda row: [(word, 1) for word in row['body'].split() if word not in broadcast_stop_words.value]\n",
    "    ).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    # Filter out words that appear less than 10 times\n",
    "    filtered_word_count_rdd = word_count_rdd.filter(lambda x: x[1] >= 10).cache()\n",
    "    filtered_word_score_rdd = word_score_rdd.join(filtered_word_count_rdd).mapValues(lambda x: x[0] / x[1])\n",
    "\n",
    "    # Sort the results by score and take the top 10\n",
    "    top_10_words_rdd = filtered_word_score_rdd.sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "\n",
    "    # Print the results\n",
    "    for word, score in top_10_words_rdd:\n",
    "        print(f\"{word}: {score}\")\n",
    "    end_time = time.time()\n",
    "    print('Spend:', end_time-start_time,'s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5224d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def without_pyspark_function(data):\n",
    "    word_score = {}\n",
    "    word_count = {}\n",
    "    print('Without using Pyspark')\n",
    "    start_time = time.time()\n",
    "    data = data.loc[:, ['score', 'body']]\n",
    "    data = data[data['body'].str.len() >= 10] # drop len(body) less than 10 \n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        score = row['score']\n",
    "        words_list = row['body'].split()\n",
    "        for word in words_list:\n",
    "            # filter stop word\n",
    "            if word not in stop_words:\n",
    "                if word not in word_count.keys():\n",
    "                    word_count[word] = 1\n",
    "                    word_score[word] = score\n",
    "                else:\n",
    "                    word_count[word] = 1 + word_count[word]\n",
    "                    word_score[word] = score + word_score[word]\n",
    "\n",
    "    filtered_word_count = {k: v for k, v in word_count.items() if v >= 10}\n",
    "    filtered_word_score = {k: v for k, v in word_score.items() if k in filtered_word_count.keys()}\n",
    "\n",
    "    for word,times in filtered_word_count.items():\n",
    "        if filtered_word_score[word] != 0:\n",
    "            filtered_word_score[word] = filtered_word_score[word]/times\n",
    "        else:\n",
    "            pass    \n",
    "\n",
    "    sorted_filtered_word_score = dict(sorted(filtered_word_score.items(), key=lambda x: x[1], reverse=True))\n",
    "    # top 10 word  that may let your comments get a higher score in Reddit\n",
    "    for key, value in list(sorted_filtered_word_score.items())[:10]:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print('Spend:', end_time-start_time,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edb0009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without using Pyspark\n",
      "fantastic,: 80.81818181818181\n",
      "famous.: 80.2\n",
      "a:: 68.2\n",
      "fundamentalism,: 62.45454545454545\n",
      "times): 58.25\n",
      "liquids: 56.07142857142857\n",
      "analytical: 47.05882352941177\n",
      "===========================: 47.0\n",
      "vocational: 46.80769230769231\n",
      "creationism.: 45.86666666666667\n",
      "Spend: 9.641444683074951 s\n"
     ]
    }
   ],
   "source": [
    "without_pyspark_function(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75bb0fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pyspark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fantastic,: 80.81818181818181\n",
      "famous.: 80.2\n",
      "a:: 68.2\n",
      "fundamentalism,: 62.45454545454545\n",
      "times): 58.25\n",
      "liquids: 56.07142857142857\n",
      "analytical: 47.05882352941177\n",
      "===========================: 47.0\n",
      "vocational: 46.80769230769231\n",
      "creationism.: 45.86666666666667\n",
      "Spend: 15.22739839553833 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "use_pyspark_function(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b25d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629a216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c6f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa75c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada3f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
