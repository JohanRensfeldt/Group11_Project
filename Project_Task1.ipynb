{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31c753a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task: \n",
    "## (1) Which keywords can let your comments get a higher score in Reddit? \n",
    "## 2. Compared the run time of (1) in common way and pyspark\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import os\n",
    "from pyspark.sql.functions import length\n",
    "from Pre_processing import data  # import data from Pre_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b71de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation for Task(1) and (2)\n",
    "start_time = time.time()\n",
    "stop_words = ['a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it',\n",
    "              'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they',\n",
    "              'this', 'to', 'was', 'will', 'with', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', \n",
    "              '_', '+', '=', '[', ']', '{', '}', ';', ':', '\"', \"'\", '<', '>', ',', '.', '/', '?', '\\\\', \n",
    "              '|', '`', '~', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40cfc2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_function(data, use_pyspark=False):\n",
    "    start_time = time.time()\n",
    "    if use_pyspark:\n",
    "        print('Using Pyspark')\n",
    "        spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.70:7077\") \\\n",
    "        .appName(\"yufengchen_app\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.cores.max\", 4)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "        data_pyspark = spark_session.createDataFrame(data)\n",
    "        data_pyspark = data_pyspark.filter((length(data_pyspark.body) >= 10) & (data_pyspark['body'] != '[deleted]'))\n",
    "        data = data_pyspark.select('score', 'body')\n",
    "        word_score = {}\n",
    "        word_count = {}\n",
    "        data_collect = data.collect()\n",
    "        for row in data_collect:\n",
    "            score = row['score']\n",
    "            words_list = row['body'].split()\n",
    "            for word in words_list:\n",
    "                # filter stop word\n",
    "                if word not in stop_words:\n",
    "                    if word not in word_count.keys():\n",
    "                        word_count[word] = 1\n",
    "                        word_score[word] = score\n",
    "                    else:\n",
    "                        word_count[word] = 1 + word_count[word]\n",
    "                        word_score[word] = score + word_score[word]\n",
    "        \n",
    "    else:\n",
    "        print('Without using Pyspark')\n",
    "        data = data.loc[:, ['score', 'body']]\n",
    "        data = data[data['body'].str.len() >= 10] # drop len(body) less than 10 \n",
    "        data = data.drop(data[data['body'] == '[deleted]'].index)  # drop body that deleted\n",
    "        word_score = {}\n",
    "        word_count = {}\n",
    "        \n",
    "        for index, row in data.iterrows():\n",
    "            score = row['score']\n",
    "            words_list = row['body'].split()\n",
    "            for word in words_list:\n",
    "                # filter stop word\n",
    "                if word not in stop_words:\n",
    "                    if word not in word_count.keys():\n",
    "                        word_count[word] = 1\n",
    "                        word_score[word] = score\n",
    "                    else:\n",
    "                        word_count[word] = 1 + word_count[word]\n",
    "                        word_score[word] = score + word_score[word]\n",
    "\n",
    "    # filter the word appear less than 5 times\n",
    "    filtered_word_count = {k: v for k, v in word_count.items() if v >= 5}\n",
    "    filtered_word_score = {k: v for k, v in word_score.items() if k in filtered_word_count.keys()}\n",
    "    \n",
    "    for word,times in filtered_word_count.items():\n",
    "        if filtered_word_score[word] != 0:\n",
    "            filtered_word_score[word] = filtered_word_score[word]/times\n",
    "        else:\n",
    "            pass    \n",
    "\n",
    "    sorted_filtered_word_score = dict(sorted(filtered_word_score.items(), key=lambda x: x[1], reverse=True))\n",
    "    # top 10 word  that may let your comments get a higher score in Reddit\n",
    "    for key, value in list(sorted_filtered_word_score.items())[:10]:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    if use_pyspark:\n",
    "        print('Spend:', end_time-start_time,'s')\n",
    "    else:\n",
    "        print('Spend:', end_time-start_time,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edb0009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pyspark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viaweb: 19.5\n",
      "books.: 11.5\n",
      "money,: 10.5\n",
      "spoken: 9.6\n",
      "Stossel: 9.4\n",
      "about,: 9.2\n",
      "loved: 8.833333333333334\n",
      "efficient: 8.8\n",
      "You've: 8.6\n",
      "technical: 8.5\n",
      "Spend: 5.218137502670288 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "task_function(data, use_pyspark=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75bb0fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without using Pyspark\n",
      "Viaweb: 19.5\n",
      "books.: 11.5\n",
      "money,: 10.5\n",
      "spoken: 9.6\n",
      "Stossel: 9.4\n",
      "about,: 9.2\n",
      "loved: 8.833333333333334\n",
      "efficient: 8.8\n",
      "You've: 8.6\n",
      "technical: 8.5\n",
      "Spend: 0.25510096549987793 s\n"
     ]
    }
   ],
   "source": [
    "task_function(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b25d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629a216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
