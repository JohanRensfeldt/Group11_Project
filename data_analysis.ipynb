{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945e2218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "org.mongodb#mongo-java-driver added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-28a10b31-2642-45cd-a43d-553a669f53ef;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.12.12 in central\n",
      ":: resolution report :: resolve 266ms :: artifacts dl 27ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.12.12 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-28a10b31-2642-45cd-a43d-553a669f53ef\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/14ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 19:29:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length, split, explode, sum, count, avg, col\n",
    "from pyspark.sql.functions import regexp_replace, lower\n",
    "spark = SparkSession.\\\n",
    "builder.\\\n",
    "appName(\"mongotest\").\\\n",
    "master(\"spark://spark-master:7077\").\\\n",
    "config(\"spark.worker.webui.port\", \"8081\").\\\n",
    "config(\"spark.executor.memory\", \"5g\").\\\n",
    "config(\"spark.executor.cores\", \"3\").\\\n",
    "config(\"spark.submit.deployMode\", \"client\"). \\\n",
    "config(\"spark.driver.bindAddress\", \"0.0.0.0\"). \\\n",
    "config(\"spark.mongodb.input.uri\",\"mongodb://admin:1989@mongoDb:27017/reddit.February?authSource=admin\").\\\n",
    "config(\"spark.mongodb.output.uri\",\"mongodb://admin:1989@mongoDb:27017/reddit.February?authSource=admin\").\\\n",
    "config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.mongodb:mongo-java-driver:3.12.12\").\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3645688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 19:29:10 WARN MongoInferSchema: Field 'edited' contains conflicting types converting to StringType\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 19:29:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"mongo\").load().filter((length('body') >= 5) & (~(col('body') == '[deleted]')))\n",
    "data = df.withColumn('body', lower(regexp_replace('body', \"[^a-zA-Z\\\\s]\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d873c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import os\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "stop_words = ['a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it',\n",
    "              'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they',\n",
    "              'this', 'to', 'was', 'will', 'with', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', \n",
    "              '_', '+', '=', '[', ']', '{', '}', ';', ':', '\"', \"'\", '<', '>', ',', '.', '/', '?', '\\\\', \n",
    "              '|', '`', '~', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b12ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_pyspark_function(data, spark_session=spark):\n",
    "    word_score = {}\n",
    "    word_count = {}\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"ERROR\")  \n",
    "    data_pyspark = data\n",
    "    print('Using Pyspark')\n",
    "    start_time = time.time()                                                                                           \n",
    "    data_pyspark = data_pyspark.select('score', 'body') \n",
    "    \n",
    "    broadcast_stop_words = spark_context.broadcast(stop_words)\n",
    "\n",
    "    # Compute the word score and count RDDs\n",
    "    word_score_rdd = data_pyspark.rdd.flatMap(\n",
    "        lambda row: [(word, row['score']) for word in row['body'].split() if word not in broadcast_stop_words.value]\n",
    "    ).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    word_count_rdd = data_pyspark.rdd.flatMap(\n",
    "        lambda row: [(word, 1) for word in row['body'].split() if word not in broadcast_stop_words.value]\n",
    "    ).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    # Filter out words that appear less than 10 times\n",
    "    filtered_word_count_rdd = word_count_rdd.filter(lambda x: x[1] >= 10).cache()\n",
    "    filtered_word_score_rdd = word_score_rdd.join(filtered_word_count_rdd).mapValues(lambda x: x[0] / x[1])\n",
    "\n",
    "    # Sort the results by score and take the top 10\n",
    "    top_10_words_rdd = filtered_word_score_rdd.sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "\n",
    "    # Print the results\n",
    "    for word, score in top_10_words_rdd:\n",
    "        print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e4a0881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pyspark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================================>(1225 + 1) / 1226]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltedited: 4111.2\n",
      "revealingly: 3287.785714285714\n",
      "unwo: 1802.4166666666667\n",
      "backbottom: 1749.5454545454545\n",
      "gtkatie: 1714.1\n",
      "enoughthen: 1528.0\n",
      "usgt: 1467.0\n",
      "peiter: 1457.6\n",
      "arachnophilia: 1452.9444444444443\n",
      "nonwedding: 1399.6666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "use_pyspark_function(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d63e83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d4bf21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
